
<!doctype html>
<html class="no-js" lang="">

<head>
  <meta charset="utf-8">
  <title>DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition</title>
  <meta name="description" content="DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="manifest" href="site.webmanifest">
  <link rel="apple-touch-icon" href="icon.png">

  <link rel="icon" href="favicon.ico">

  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/main.css">

  <meta name="theme-color" content="#fafafa">
</head>

<body>
  <!--[if IE]>
	<p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <div class="wrapper">
   <header>
     <teaser>
     </teaser>
     <h1>
      <b>DxHF</b>: 
      <span>Providing High-Quality Human Feedback <br> for LLM Alignment via Interactive Decomposition<br>
      </span>
      <p style="font-size:medium;"><a href="https://uist.acm.org/2025/">UIST 2025 @ Busan ðŸ‡°ðŸ‡·</a></p>
    </h1>
  </header>
  <section>
    <h2>Authors</h2>
    <a href="https://sdq.github.io" target="_blank">Danqing Shi</a><span class="sep"></span>
    <a href="https://www.furuicheng.tech" target="_blank">Furui Cheng</a><span class="sep"></span>
    <a href="https://www.kth.se/profile/weinkauf" target="_blank">Tino Weinkauf</a><span class="sep"></span>
    <a href="https://users.aalto.fi/~oulasvir/" target="_blank">Antti Oulasvirta</a><span class="sep"></span>
    <a href="https://el-assady.com/" target="_blank">Mennatallah El-Assady</a>
    <img class="teaser" src="img/affilitations.png" />
  </section>
  <section>
    <h2>Abstract</h2>
    <p class="abstract">
      Human preferences are widely used to align large language models (LLMs) through methods such as reinforcement learning from human feedback (RLHF). However, the current user interfaces require annotators to compare text paragraphs, which is cognitively challenging when the texts are long or unfamiliar. This paper contributes by studying the decomposition principle as an approach to improving the quality of human feedback for LLM alignment. This approach breaks down the text into individual claims instead of directly comparing two long-form text responses. Based on the principle, we build a novel user interface DxHF. It enhances the comparison process by showing decomposed claims, visually encoding the relevance of claims to the conversation and linking similar claims. This allows users to skim through key information and identify differences for better and quicker judgment. Our technical evaluation shows evidence that decomposition generally improves feedback accuracy regarding the ground truth, particularly for users with uncertainty. A crowdsourcing study with 160 participants indicates that using DxHF improves feedback accuracy by an average of 5%, although it increases the average feedback time by 18 seconds. Notably, accuracy is significantly higher in situations where users have less certainty. The finding of the study highlights the potential of HCI as an effective method for improving human-AI alignment. 
   </p>
   <img class="teaser" src="img/teaser.png" />
 </section>
 <!-- <section>
  <img class="teaser" src="img/algorithm.png" />
 </section> -->
 <section>
  <h2>Media</h2>
  <video width="750" controls>
    <source src="./video/VideoFigure.mp4" type="video/mp4">
  </video>
</section>
<section>
  <h2>Material</h2>
  <a href="">Paper</a><span class="sep"></span>
  <a href="">Demo</a><span class="sep"></span>
  <a href="https://github.com/sdq/DxHF" target="_blank">Code</a>
</section>
<section>
  <h2>Related Projects</h2>
  <a href="https://sdq.github.io/rewardvis" target="_blank">Interactive Reward Tuning</a><span class="sep"></span>
  <a href="https://jankomp.github.io/interactive_rlhf" target="_blank">Interactive-RLHF</a>
</section>
<section>
    <h2>Cite</h2>

  <div class="citation-container">
    <div class="citation">@inproceedings{shi2025dxhf,
      title={DxHF: Providing High-Quality Human Feedback for LLM Alignment via Interactive Decomposition},
      author = {Shi, Danqing and Cheng, Furui and Weinkauf, Tino and Oulasvirta, Antti and El-Assady, Mennatallah},
      publisher = {Association for computing Machinery},
      booktitle = {Proceedings of the 38th annual acm symposium on user interface software and technology},
      year={2025},
      doi={10.1145/3746059.3747600}
    }</div>
    <!-- <span class="citation-title">BibteX</span> -->
  </div>
  
  <div class="block app-block-footer text-center">
    <footer class="footer">
      <p>&copy; Website is designed by <a href="https://sdq.github.io" target="_blank">Danqing Shi</a></p>
    </footer>
  </div>
</div>
</body>

</html>
